# ğŸ§  Proyecto: IntegraciÃ³n de Kafka y Spark Streaming  
**Autor:** Jhon IvÃ¡n Forero  
**Curso:** Big Data â€“ UNAD 2025  
**Anexo 3:** Spark Streaming y Kafka  

---

## ğŸ“˜ IntroducciÃ³n  
Este proyecto corresponde al desarrollo del **Anexo 3: Spark Streaming y Kafka**, cuyo propÃ³sito fue implementar un flujo de datos en tiempo real utilizando **Apache Kafka** como plataforma de mensajerÃ­a y **Apache Spark Streaming** como motor de procesamiento.  
El objetivo principal fue demostrar cÃ³mo ambas herramientas trabajan de forma integrada para procesar grandes volÃºmenes de datos en tiempo real, cumpliendo con el **RAC 2 del curso**: diseÃ±ar e implementar soluciones de almacenamiento y procesamiento de grandes volÃºmenes de datos con Hadoop, Spark y Kafka.

---

## ğŸ¯ Objetivos  

### **Objetivo general**  
DiseÃ±ar e implementar una soluciÃ³n de procesamiento de datos en tiempo real utilizando Apache Kafka y Spark Streaming.

### **Objetivos especÃ­ficos**
1. Configurar e iniciar los servicios de Kafka y ZooKeeper.  
2. Desarrollar un productor en Python para generar datos simulados de sensores.  
3. Implementar un consumidor con Spark Streaming para procesar y analizar los datos en tiempo real.  
4. Validar la conexiÃ³n y el flujo de datos entre Kafka y Spark.  
5. Documentar el proceso y compartir el cÃ³digo en un repositorio GitHub.

---

## âš™ï¸ Requisitos previos  
- **Java 17 o superior**  
- **Python 3.10+**  
- **Apache Kafka 3.6.2**  
  > [Descarga desde Apache Archive](https://archive.apache.org/dist/kafka/3.6.2/kafka-3.6.2-src.tgz)  
- **Apache Spark 3.5.6 (con Hadoop 3)**  
  > [Descarga oficial](https://archive.apache.org/dist/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz)  
- LibrerÃ­as de Python necesarias:  
  ```bash
  pip install kafka-python pyspark
  ```

---

## ğŸ§© Estructura del repositorio
```
â”œâ”€â”€ kafka_producer.py              # Script productor: genera datos de sensores
â”œâ”€â”€ spark_streaming_consumer.py    # Script consumidor: procesa los datos en Spark
â”œâ”€â”€ README.md                      # Documento de descripciÃ³n e instrucciones
```

---

## ğŸš€ Instrucciones de ejecuciÃ³n

### 1ï¸âƒ£ Iniciar los servicios
```bash
sudo /opt/Kafka/bin/zookeeper-server-start.sh -daemon /opt/Kafka/config/zookeeper.properties
sudo /opt/Kafka/bin/kafka-server-start.sh -daemon /opt/Kafka/config/server.properties
```

### 2ï¸âƒ£ Crear el topic
```bash
/opt/Kafka/bin/kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 1 \
  --topic sensor_data
```

### 3ï¸âƒ£ Ejecutar el productor
```bash
python3 kafka_producer.py
```

### 4ï¸âƒ£ Ejecutar el consumidor en Spark Streaming
```bash
python3 spark_streaming_consumer.py
```

---

## ğŸ§  SoluciÃ³n de errores
- **Error:** `NoBrokersAvailable`  
  âœ… **SoluciÃ³n:** iniciar los servicios de ZooKeeper y Kafka antes de ejecutar el productor.
- **Error:** `Failed to find data source: kafka`  
  âœ… **SoluciÃ³n:** iniciar PySpark con el paquete Kafka.  
  ```bash
  $SPARK_HOME/bin/pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6
  ```

---

## ğŸ“Š Resultados
- El **productor** envÃ­a datos simulados de sensores (ID, temperatura, humedad, timestamp).  
- El **consumidor** procesa y agrupa los datos en tiempo real, mostrando estadÃ­sticas por minuto.  
- Se validÃ³ la conectividad entre Kafka y Spark, confirmando el flujo completo de datos.  

---

## ğŸ¥ Video demostrativo  
> ğŸ¬ *Enlace al video explicativo en YouTube o Google Drive*  

---

## ğŸ§¾ Referencias
- Apache Kafka Documentation â€“ https://kafka.apache.org/documentation/  
- Apache Spark Structured Streaming â€“ https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html  
- UNAD (2025). *GuÃ­a de aprendizaje â€“ Tarea 3: Procesamiento de Datos con Apache Spark*  
- Maldonado, S. V. (2022). *Analytics y Big Data*. RIL Editores.

---

## âœ… ConclusiÃ³n  
Este proyecto permitiÃ³ afianzar el conocimiento sobre **procesamiento de datos en tiempo real**, demostrando la interacciÃ³n entre **Kafka y Spark Streaming**. La prÃ¡ctica mostrÃ³ cÃ³mo un sistema puede capturar, transmitir y procesar datos de manera continua, siendo una base sÃ³lida para proyectos de Big Data a nivel empresarial o acadÃ©mico.
